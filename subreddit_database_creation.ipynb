{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Reddit Data\n",
    "\n",
    "The notebook demonstrates how to extract massive amounts of reddit data in order to build a map of subreddits. \n",
    "\n",
    "Data extracted from https://files.pushshift.io/reddit/comments/\n",
    "- Files are around 2-10 times larger when uncompressed\n",
    "- Uncompressed files must be renamed to have .json file extension\n",
    "\n",
    "Code adapted from https://github.com/lmcinnes/subreddit_mapping\n",
    "\n",
    "Credit to reddit user /u/Stuck_In_the_Matrix and /u/hoffa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "import adjustText\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "from os.path import isfile\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value, CustomJS, DataRange1d\n",
    "from bokeh.models.mappers import LinearColorMapper\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.palettes import plasma\n",
    "from bokeh.events import MouseWheel\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build SQL Database\n",
    "\n",
    "Since the dataset is huge (71,826,552 comments), we need to read and write to the SQL database in chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_objects_in_file(filepath):\n",
    "    idx = 0\n",
    "    with open(filepath) as fp:\n",
    "        for line in fp:\n",
    "            idx+=1\n",
    "    print(\"File has {} lines\".format(idx))\n",
    "\n",
    "\n",
    "def extract_subset(filepath, start=0, end=10):\n",
    "    comments = []\n",
    "    with open(filepath) as fp:\n",
    "        for idx, line in enumerate(fp):\n",
    "            if (idx >= start) and (idx < end):\n",
    "                comment = json.loads(line)\n",
    "                comments.append(comment)\n",
    "            elif idx >= end:\n",
    "                break                \n",
    "        return comments\n",
    "        \n",
    "    \n",
    "def write_to_database(db_conn, json_fp, chunk_size):\n",
    "    batch_no=1\n",
    "    for chunk in pd.read_json(json_fp, chunksize=chunk_size, lines=True):\n",
    "        try: \n",
    "            chunk.to_sql('reddit_comments', db_conn, if_exists='append')\n",
    "        except sqlalchemy.exc.SQLAlchemyError as e: \n",
    "            print(\"\\n  {}\".format(e.orig))\n",
    "        print('\\rindex: {}'.format(batch_no), end='')\n",
    "        batch_no+=1\n",
    "        \n",
    "        \n",
    "def create_database(database, json_fp, n_files, comments_per_file, chunk_size, columns_to_drop):\n",
    "    for idx in range(0, n_files):\n",
    "        print(\"########## File {} ##########\".format(idx+1))\n",
    "        start = int(idx*comments_per_file)\n",
    "        end = int(start + comments_per_file)\n",
    "        print(\"Extracting comments {} - {}\".format(start, end))\n",
    "        comments = extract_subset(filepath=json_fp, start=start, end=end)\n",
    "        df = pd.DataFrame(comments)\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        if \"media_metadata\" in df.columns:\n",
    "            df = df.drop(columns=[\"media_metadata\"])\n",
    "        df.to_json(\"data/db_chunk.json\", orient='records', lines=True)\n",
    "\n",
    "        print(\"Writing to database\")\n",
    "        write_to_database(\n",
    "            db_conn=database, \n",
    "            json_fp=\"data/db_chunk.json\", \n",
    "            chunk_size=chunk_size\n",
    "        )\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        \n",
    "def run_database_builder(input_path, db_path, drop_cols, n_millions):\n",
    "    \n",
    "    conn = create_engine('sqlite:///'+db_path)\n",
    "\n",
    "    create_database(\n",
    "        database=conn, \n",
    "        json_fp=input_path,\n",
    "        n_files=n_millions, \n",
    "        comments_per_file=1000000, \n",
    "        chunk_size=100000,\n",
    "        columns_to_drop=drop_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_objects_in_file(filepath=\"data/RC_2017-10.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RC_2017_10 has 85,828,912 comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes to keep\n",
    "'subreddit_id'\n",
    "'link_id'\n",
    "'subreddit'\n",
    "'created_utc'\n",
    "'retrieved_on'\n",
    "'author_flair_text'\n",
    "'score'\n",
    "'controversiality'\n",
    "'author'\n",
    "'edited'\n",
    "'id'\n",
    "'gilded'\n",
    "'parent_id'\n",
    "'body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# comments = extract_subset(filepath=\"data/RC_2017-10.json\", start=0, end=100)\n",
    "# df = pd.DataFrame(comments)\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"stickied\", \"distinguished\", \"author_flair_css_class\", \n",
    "             \"can_gild\", \"is_submitter\", \"permalink\", \"author_cakeday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_database_builder(\n",
    "    input_path=\"data/RC_2017-10.json\", \n",
    "    db_path=\"databases/RC_2017_10_database.db\", \n",
    "    drop_cols=drop_cols,\n",
    "    n_millions=85,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"databases/RC_2017_10_database.db\"\n",
    "conn = sqlite3.connect(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"SELECT * FROM reddit_comments LIMIT 100\"\"\", conn)\n",
    "df = df.drop(columns=[\"index\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of number of users in each subreddit, and save the result a new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY1 = \"\"\"\n",
    "CREATE TABLE subr_users AS\n",
    "    SELECT subreddit, authors, DENSE_RANK() OVER (ORDER BY authors DESC) AS rank_authors\n",
    "    FROM (SELECT subreddit, SUM(1) as authors\n",
    "         FROM (SELECT subreddit, author, COUNT(1) as cnt \n",
    "             FROM reddit_comments\n",
    "             WHERE author NOT LIKE '%bot'\n",
    "             GROUP BY subreddit, author HAVING cnt > 0)\n",
    "         GROUP BY subreddit) t\n",
    "    ORDER BY authors DESC;\n",
    "\"\"\"\n",
    "\n",
    "c = conn.cursor()\n",
    "c.execute(QUERY1)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = conn.cursor()\n",
    "# c.execute(\"\"\"DROP TABLE subr_users_v2\"\"\")\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"SELECT * FROM subr_users\"\"\", conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"authors\")\n",
    "plt.plot(df.index, df.authors)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"authors hist\")\n",
    "plt.hist(df.authors, bins=50)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.title(\"authors\")\n",
    "plt.plot(df.index[200:2201], df.authors[200:2201])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the new table, we create a list of number of users who authored at least 10 posts in pairs of subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY2 = \"\"\"\n",
    "CREATE TABLE overlapping_subr_users AS\n",
    "    SELECT t1.subreddit, t2.subreddit, SUM(1) AS NumOverlaps\n",
    "    FROM (SELECT subreddit, author, COUNT(1) AS cnt \n",
    "         FROM reddit_comments\n",
    "         WHERE author NOT LIKE '%bot'\n",
    "         AND subreddit IN (SELECT subreddit FROM subr_users\n",
    "           WHERE rank_authors>200 AND rank_authors<2201)\n",
    "         GROUP BY subreddit, author HAVING cnt > 10) t1\n",
    "    JOIN (SELECT subreddit, author, COUNT(1) as cnt \n",
    "         FROM reddit_comments\n",
    "         WHERE author NOT LIKE '%bot'\n",
    "         GROUP BY subreddit, author HAVING cnt > 10) t2\n",
    "    ON t1.author=t2.author\n",
    "    WHERE t1.subreddit!=t2.subreddit\n",
    "    GROUP BY t1.subreddit, t2.subreddit\n",
    "\"\"\"\n",
    "\n",
    "c = conn.cursor()\n",
    "c.execute(QUERY2)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.execute(\"\"\"DROP TABLE overlapping_subr_users_v2\"\"\")\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the result of the second query as a dataframe. Edit the column names and store it for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"SELECT * FROM overlapping_subr_users\"\"\", conn)\n",
    "df = df.rename(columns={\"subreddit\":\"t1_subreddit\", \"subreddit:1\":\"t2_subreddit\"})\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"t1_subreddit unique subreddits: {}\".format(len(df[\"t1_subreddit\"].unique())))\n",
    "print(\"t2_subreddit unique subreddits: {}\".format(len(df[\"t2_subreddit\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/RC_2017_10_subreddit_overlaps.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/RC_2017_10_subreddit_overlaps.csv\")\n",
    "# raw_data = pd.read_csv(\"data/subreddit_overlaps_BQ.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of pairwise commenter overlaps: {}\".format(len(raw_data)))\n",
    "print(\"t1_subreddit unique subreddits: {}\".format(len(raw_data[\"t1_subreddit\"].unique())))\n",
    "print(\"t2_subreddit unique subreddits: {}\".format(len(raw_data[\"t2_subreddit\"].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank the subreddits so that they are indexed in order of popularity. Popularity is defined by the total number of unique commenters in each subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_popularity = raw_data.groupby('t2_subreddit')['NumOverlaps'].sum()\n",
    "subreddits = np.array(subreddit_popularity.sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.tolist()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot the data into a matrix such that rows and columns are both indexed by subreddits, and the entry at position (i,j) is the number of overlaps bwteen the ith and jth subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subreddit-to-integer-index map to convert the subreddit names in the table into numeric row and column indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map = dict(np.vstack([subreddits, np.arange(subreddits.shape[0])]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = raw_data.NumOverlaps\n",
    "row_indices = raw_data.t2_subreddit.map(index_map)\n",
    "col_indices = raw_data.t1_subreddit.map(index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sparse matrix. This format requires us to specify triples of row, column, and value for each non-zero entry in the matrix. The COO matrix constructor accepts this as a triple of arrays: the first array is the values, the second and third are arrays of row and column indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = ss.coo_matrix((values, (row_indices,col_indices)),\n",
    "                              shape=(subreddits.shape[0], subreddits.shape[0]),\n",
    "                              dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_prob_matrix = count_matrix.tocsr()\n",
    "conditional_prob_matrix = normalize(conditional_prob_matrix, norm='l1', copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting subreddit vectors into a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear dimensionality reduction down to 500 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors = TruncatedSVD(n_components=500, random_state=1).fit_transform(conditional_prob_matrix)\n",
    "reduced_vectors = normalize(reduced_vectors, norm='l2', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear dimensionality reduction down to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeVis(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_components=2, perplexity=30.0, gamma=5,\n",
    "                 layout_samples=None, n_neighbors=None, negative_samples=5,\n",
    "                 alpha=1.0, n_cores=6, knn_prop=3, trees=50):\n",
    "        self.n_components = n_components\n",
    "        self.perplexity = perplexity\n",
    "        self.layout_samples = layout_samples\n",
    "        self.alpha = alpha\n",
    "        self.n_cores = n_cores\n",
    "        self.knn_prop = knn_prop\n",
    "        self.negative_samples = negative_samples\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.gamma = gamma\n",
    "        self.trees = trees\n",
    "        if self.n_neighbors is None:\n",
    "            self.n_neighbors = int(self.perplexity * 3)\n",
    "\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \n",
    "        if self.layout_samples is None:\n",
    "            layout_samples = X.shape[0] / 100.0\n",
    "        else:\n",
    "            layout_samples = self.layout_samples\n",
    "            \n",
    "        X = check_array(X, dtype=np.float64)\n",
    "        np.savetxt('/tmp/largevis_input', \n",
    "                   X, header='{} {}'.format(*X.shape), \n",
    "                   comments='')\n",
    "        subprocess.check_call(['/Users/cameronlaedtke/LargeVis-python3/Linux/LargeVis',\n",
    "                               '-input', '/tmp/largevis_input',\n",
    "                               '-output', '/tmp/largevis_output',\n",
    "                               '-outdim', str(self.n_components),\n",
    "                               '-perp', str(self.perplexity),\n",
    "                               '-samples', str(layout_samples),\n",
    "                               '-gamma', str(self.gamma),\n",
    "                               '-prop', str(self.knn_prop),\n",
    "                               '-trees', str(self.trees),\n",
    "                               '-neigh', str(self.n_neighbors),\n",
    "                               '-alpha', str(self.alpha),\n",
    "                               '-neg', str(self.negative_samples),\n",
    "                               '-threads', str(self.n_cores)])\n",
    "        self.embedding_ = np.loadtxt('/tmp/largevis_output', skiprows=1)\n",
    "        return self.embedding_\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_transform(X)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = 'tsne_data/RC_2017_10_largevis_subreddit_map.npy'\n",
    "if isfile(embed_file):\n",
    "    subreddit_map = np.load(embed_file)\n",
    "else:\n",
    "    largevis = LargeVis(perplexity=30, n_cores=12)\n",
    "    subreddit_map = largevis.fit_transform(reduced_vectors[:10000])\n",
    "    np.save(embed_file, subreddit_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largevis = LargeVis(perplexity=30, n_cores=12)\n",
    "# subreddit_map = largevis.fit_transform(reduced_vectors[:10000])\n",
    "# np.save(embed_file, subreddit_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_map_df = pd.DataFrame(subreddit_map, columns=('x', 'y'))\n",
    "subreddit_map_df['subreddit'] = subreddits[:10000]\n",
    "subreddit_map_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = HDBSCAN(min_samples=5, min_cluster_size=20).fit(subreddit_map)\n",
    "cluster_ids = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_map_df['cluster'] = cluster_ids\n",
    "subreddit_map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster_points = len(subreddit_map_df[subreddit_map_df.cluster != -1])\n",
    "n_clusters = subreddit_map_df[\"cluster\"].max()\n",
    "score = silhouette_score(subreddit_map, cluster_ids)\n",
    "print(\"points assigned to a cluster: {}\".format(n_cluster_points))\n",
    "print(\"number of clusters: {}\".format(n_clusters))\n",
    "print(\"silhouette score: {:.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Construct a color palette and map clusters to colors\n",
    "palette = ['#777777'] + list(plasma(cluster_ids.max()))\n",
    "colormap = LinearColorMapper(palette=palette, low=-1, high=cluster_ids.max())\n",
    "color_dict = {'field': 'cluster', 'transform': colormap}\n",
    "\n",
    "# Set fill alpha globally\n",
    "subreddit_map_df['fill_alpha'] = np.exp((subreddit_map.min() - subreddit_map.max()) / 5.0) + 0.05\n",
    "\n",
    "# Build a column data source\n",
    "plot_data = ColumnDataSource(data=subreddit_map_df)\n",
    "\n",
    "# Create the figure and add tools\n",
    "fig = figure(\n",
    "    title='A Map of Subreddits',\n",
    "    plot_width = 1000,\n",
    "    plot_height = 1000,\n",
    "    tools= ('pan, wheel_zoom, box_zoom,''box_select, reset'),\n",
    "    active_scroll=u'wheel_zoom',\n",
    ")\n",
    "\n",
    "fig.add_tools(HoverTool(tooltips = OrderedDict([('subreddit', '@subreddit'), \n",
    "                                                ('cluster', '@cluster')])))\n",
    "\n",
    "# draw the subreddits as circles on the plot\n",
    "fig.circle(\n",
    "    u'x', u'y', \n",
    "    source = plot_data,\n",
    "    fill_color = color_dict, \n",
    "    line_color = None, \n",
    "    fill_alpha = 'fill_alpha',\n",
    "    size = 10, \n",
    "    hover_line_color = u'black'\n",
    ")\n",
    "\n",
    "# Custom callback for alpha adjustment\n",
    "jscode=\"\"\"\n",
    "    var data = source.data;\n",
    "    var start = cb_obj.start;\n",
    "    var end = cb_obj.end;\n",
    "    var alpha = data['fill_alpha']\n",
    "    for (var i = 0; i < alpha.length; i++) {\n",
    "         alpha[i] = Math.exp((start - end) / 5.0) + 0.05;\n",
    "    }\n",
    "    source.change.emit();\n",
    "\"\"\"\n",
    "\n",
    "callback = CustomJS(args=dict(source=plot_data), code=jscode)\n",
    "\n",
    "fig.x_range.js_on_change(\"start\", callback)\n",
    "fig.x_range.js_on_change(\"end\", callback)\n",
    "\n",
    "# configure visual elements of the plot\n",
    "fig.title.text_font_size = value('18pt')\n",
    "fig.title.align = 'center'\n",
    "fig.xaxis.visible = False\n",
    "fig.yaxis.visible = False\n",
    "fig.grid.grid_line_color = None\n",
    "fig.outline_line_color = '#222222'\n",
    "\n",
    "# display the figure\n",
    "output_file('viz/subreddit_interactive_map.html')\n",
    "show(fig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
