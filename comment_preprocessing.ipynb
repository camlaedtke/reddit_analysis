{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQo6Swey4tBi"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "This notebook will generate the training, validation, and test sets that we should all use with our models. Here's some general notes on the process. It also includes all the visualization.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "The data cleaning here isn't perfect. There are instances where words like \"don't\" become \"don t\" and stuff like that. But overall it's good enough for now. Here's a good tutorial to get an idea of some of the important steps \n",
    "\n",
    "https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb\n",
    "\n",
    "\n",
    "\n",
    "### Sampling\n",
    "\n",
    "We need to pay attention to exactly how we sample the comments from each subreddit. There are two important things to focus on.\n",
    "\n",
    "#### 1) Length of comments\n",
    "\n",
    "One subreddit might have a higher average comment length than another, so we want to eliminate any effect this may have on our model. \n",
    "\n",
    "Also, if the goal of our model is to distinguish a comment as coming from a \"conservative\" subreddit versus a \"liberal\" subreddit, then longer comments are better. Short comments like \"sure haha\", \"thanks!\",\"I totally agree\", ..etc. don't give any information that could help the model's prediction. \n",
    "\n",
    "Since r/democrats is much less popular than r/Conservative and r/politics, there aren't as many lengthly discussions with long comments. You can see this by running `dem_comments_filtered = filter_by_word_count(df=dem_comments, min_words=10)` and comparing it to r/politics and r/Conservative (r/democrats also has less total comments).\n",
    "\n",
    "#### 2) Size of dataset\n",
    "\n",
    "The models should be trained on a dataset which combines an equal number of \"liberal\" and \"conservative\" comments. We have 411,811 comments from r/Conservative, 77,763 from r/democrats, and 1,481,227 from r/politics. Therefore we need take an equal number from each dataset though some type of random sampling. But before we do the random sampling we need to filter the dataset so that we're working with only comments that are above some word count threshold. \n",
    "\n",
    "WIth these two points in mind, it seems like we will have more flexibility if we use r/politics as our \"liberal\" subreddit instead of r/democrats, since we can construct a dataset with a large amount of long comments. And if you take a look at r/politics vs. r/democrats, they are very similar in overall tone and political leaning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0p9vuTE4tBh"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('xtick',labelsize=18)\n",
    "plt.rc('ytick',labelsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1606878374637,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "z9gqabd_4636",
    "outputId": "9456a46d-e365-49e1-c5c4-4c98256c5b3a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1606878374638,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "6dfk8qllKWe2",
    "outputId": "41fcae6a-243e-431d-881c-118422b5b8c2"
   },
   "outputs": [],
   "source": [
    "# from psutil import virtual_memory\n",
    "# ram_gb = virtual_memory().total / 1e9\n",
    "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "# if ram_gb < 20:\n",
    "#   print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "#   print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "#   print('re-execute this cell.')\n",
    "# else:\n",
    "#   print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HgKSUHL4tBh"
   },
   "outputs": [],
   "source": [
    "comment_col_names = [\"body_text\", \"author\", \"score\", \"created_utc\", \"post_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W094iW3I4tBh"
   },
   "source": [
    "Import data and add word count column. Remove rows which are missing body_text, remove duplicate rows having same body_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKdGR0M-4tBh"
   },
   "outputs": [],
   "source": [
    "def import_comments(filename):\n",
    "    df = pd.read_csv(filename, names=comment_col_names)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df.body_text != ''] \n",
    "    df.drop_duplicates(['body_text'], inplace=True) \n",
    "    df[\"body_text\"].astype(\"string\")\n",
    "    df['body_word_count'] = df['body_text'].apply(lambda x: len(x.strip().split()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S29wO9xO4tBh"
   },
   "outputs": [],
   "source": [
    "con_comments_raw = import_comments(filename=\"reddit_data/conservative_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yG9Tw0ei4tBh"
   },
   "outputs": [],
   "source": [
    "dem_comments_raw = import_comments(filename=\"reddit_data/democrats_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D51ZL1Fx4tBh"
   },
   "outputs": [],
   "source": [
    "pol_comments_raw = import_comments(filename=\"reddit_data/politics_comments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z9kb4nL4tBh"
   },
   "source": [
    "Using the python regrex module to do some cleaning. See this link for a basic tutorial\n",
    "https://www.w3schools.com/python/python_regex.asp\n",
    "\n",
    "A couple points\n",
    "- `re.strip()` removes blank spaces from the beginning and end of the comment.\n",
    "- `re.sub()` works like this: \n",
    "\n",
    "`cleaned_comment = re.sub(r'things you want to replace', 'what you want to replace it with', comment)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bOFY--s4tBi"
   },
   "outputs": [],
   "source": [
    "punctuation='[\"\\'?,\\.]' # I will replace all these punctuation with ''\n",
    "abbr_dict={\n",
    "    \"what's\":\"what is\",\n",
    "    \"what're\":\"what are\",\n",
    "    \"who's\":\"who is\",\n",
    "    \"who're\":\"who are\",\n",
    "    \"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\n",
    "    \"when's\":\"when is\",\n",
    "    \"when're\":\"when are\",\n",
    "    \"how's\":\"how is\",\n",
    "    \"how're\":\"how are\",\n",
    "\n",
    "    \"i'm\":\"i am\",\n",
    "    \"we're\":\"we are\",\n",
    "    \"you're\":\"you are\",\n",
    "    \"they're\":\"they are\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\",\n",
    "    \"there's\":\"there is\",\n",
    "    \"there're\":\"there are\",\n",
    "\n",
    "    \"i've\":\"i have\",\n",
    "    \"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\n",
    "    \"they've\":\"they have\",\n",
    "    \"who've\":\"who have\",\n",
    "    \"would've\":\"would have\",\n",
    "    \"not've\":\"not have\",\n",
    "\n",
    "    \"i'll\":\"i will\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"you'll\":\"you will\",\n",
    "    \"he'll\":\"he will\",\n",
    "    \"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"they'll\":\"they will\",\n",
    "\n",
    "    \"isn't\":\"is not\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"aren't\":\"are not\",\n",
    "    \"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\n",
    "    \"couldn't\":\"could not\",\n",
    "    \"don't\":\"do not\",\n",
    "    \"didn't\":\"did not\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\n",
    "    \"doesn't\":\"does not\",\n",
    "    \"haven't\":\"have not\",\n",
    "    \"hasn't\":\"has not\",\n",
    "    \"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\",\n",
    "    punctuation:'',\n",
    "    '\\s+':' ', # replace multi space with one single space\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQDHRuNw4tBi"
   },
   "outputs": [],
   "source": [
    "def clean_comment(w):\n",
    "    \"\"\"This is a pretty important function. Try commenting out / uncommenting some of these lines.\"\"\"\n",
    "    w = w.strip()\n",
    "    # Take out brackets ([hello] --> hello)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z) (or add more stuff like ?,!,..etc)\n",
    "    w = re.sub(r\"[^a-zA-Z]\", \" \", w)\n",
    "    # Remove all duplicate whitespaces (\"   hello   my name   is\" --> \"hello my name is\")\n",
    "    w = \" \".join(w.split())\n",
    "    w = w.strip()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cuHyQRz4tBi"
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['body_text'] = df[\"body_text\"].str.lower()\n",
    "    df[\"body_text\"] = df[\"body_text\"].apply(lambda x: re.sub(r\"\\\\n\", \" \", x))\n",
    "    df[\"body_text\"] = df[\"body_text\"].apply(lambda x: re.sub(r\"\\\\\", \"\", x))\n",
    "    df[\"body_text\"] = df[\"body_text\"].apply(lambda x: re.sub(r'[\"?,\\.]', \"\", x))\n",
    "    # This method for replacing abbreviations only works part of the time\n",
    "    df[\"body_text\"] = df[\"body_text\"].replace(abbr_dict, regex=True, inplace=False)\n",
    "    df[\"body_text\"] = df[\"body_text\"].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "    df['body_text'] = df['body_text'].apply(lambda x: clean_comment(x)) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeLsO5QH4tBi"
   },
   "outputs": [],
   "source": [
    "con_comments = con_comments_raw.copy(deep=True)\n",
    "con_comments = clean_data(df=con_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjdYvwUk4tBi"
   },
   "outputs": [],
   "source": [
    "dem_comments = dem_comments_raw.copy(deep=True)\n",
    "dem_comments = clean_data(df=dem_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bo5UVC-w4tBi"
   },
   "outputs": [],
   "source": [
    "pol_comments = pol_comments_raw.copy(deep=True)\n",
    "pol_comments = clean_data(df=pol_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnJZmuLW4tBi"
   },
   "source": [
    "Save the final cleaned comments as `.pkl` files. Then whenever you run the notebook again you don't need to run all the data cleaning stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUiW5JOD4tBi"
   },
   "outputs": [],
   "source": [
    "con_comments.to_pickle(\"reddit_data/conservative_comments_cleaned.pkl\")\n",
    "# dem_comments.to_pickle(\"reddit_data/democrats_comments_cleaned.pkl\")\n",
    "# pol_comments.to_pickle(\"reddit_data/politics_comments_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKCtDlEr4tBi"
   },
   "source": [
    "Read in the cleaned data if it was already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqaGMMsj4tBi"
   },
   "outputs": [],
   "source": [
    "con_comments = pd.read_pickle(\"reddit_data/conservative_comments_cleaned.pkl\")\n",
    "dem_comments = pd.read_pickle(\"reddit_data/democrats_comments_cleaned.pkl\")\n",
    "pol_comments = pd.read_pickle(\"reddit_data/politics_comments_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2167,
     "status": "ok",
     "timestamp": 1606878384193,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "LkdSfRb4_6Gl",
    "outputId": "143e96dd-24f1-41d4-dd93-e5f7ec0aba0e"
   },
   "outputs": [],
   "source": [
    "print(\"{} comments from r/Conservative\".format(len(con_comments)))\n",
    "print(\"{} comments from r/democrats\".format(len(dem_comments)))\n",
    "print(\"{} comments from r/politics\".format(len(pol_comments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceotGGLCDy6c"
   },
   "source": [
    "50,000 seems like a good number to sample from each subreddit (we're using r/Conservative and r/politics). So I set the `min_words` to be the highest number that would return over 50,000 comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lxVdCQ04tBi"
   },
   "outputs": [],
   "source": [
    "def filter_by_word_count(df, min_words, max_words=False):\n",
    "    if max_words is not False:\n",
    "        df = df.loc[(df.body_word_count > min_words) & (df.body_word_count < max_words),:]\n",
    "    else:\n",
    "        df = df.loc[(df.body_word_count > min_words),:]\n",
    "    print(\"{} comments found\".format(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1606878419381,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "vwbG5P8V4tBi",
    "outputId": "714068ec-741d-49b8-eaa4-cfb669fc8e7f"
   },
   "outputs": [],
   "source": [
    "con_comments_filtered = filter_by_word_count(df=con_comments, min_words=30, max_words=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1606878420098,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "T9iCdpY04tBj",
    "outputId": "345337e3-c6d3-46c8-d731-20991b0ba1cc"
   },
   "outputs": [],
   "source": [
    "dem_comments_filtered = filter_by_word_count(df=dem_comments, min_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1606878420625,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "Fvk1Nue_4tBj",
    "outputId": "545bb716-6ecb-4d18-f75b-348d2e38d76c"
   },
   "outputs": [],
   "source": [
    "pol_comments_filtered = filter_by_word_count(df=pol_comments, min_words=30, max_words=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPvDNXJXEkDG"
   },
   "source": [
    "Randomly sample equal number from each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSkGtmQR4tBj"
   },
   "outputs": [],
   "source": [
    "con_comments = con_comments_filtered.sample(100000)\n",
    "pol_comments = pol_comments_filtered.sample(100000)\n",
    "\n",
    "con_comments = con_comments.reset_index(drop=True)\n",
    "pol_comments = pol_comments.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgnsjZW94tBj"
   },
   "source": [
    "## Handling stop words\n",
    "\n",
    "\"Stop words\" are the common words like \"the\", \"and\", \"is\" ... etc. \n",
    "\n",
    "\n",
    "\n",
    "For visualization, we need to remove them or else they will fill up the word cloud. Removing them might also help with model performance, but it depends which type of model you are using (not sure on this). Right now I am removing stop words for our final dataset, but we should revisit this and maybe train our models with stop words included. \n",
    "\n",
    "Also, from earlier versions of the word cloud I noticed certain errors in the data cleaning, like the letter \"t\" and the letter \"m\" showing up as distinct words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1606878435178,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "05HTwmo44tBj",
    "outputId": "ea661e02-d96b-48e6-8b5b-57244255c756"
   },
   "outputs": [],
   "source": [
    "# Add stuff to the stopwords list \n",
    "SW_list = list(STOPWORDS)\n",
    "# SW_list.extend([\"n\",\"want\",\"will\",\"one\",\"s\",\"dont\",\"don\",\"don t\",\"t\",\"people\",\"think\",\"even\",\"thing\",\"m\",\"u\"])\n",
    "SW_list.extend([\"n\",\"s\",\"don\",\"don t\",\"t\",\"m\",\"u\"])\n",
    "STOPWORDS = set(SW_list) \n",
    "# print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2wRhbj-FLL6"
   },
   "outputs": [],
   "source": [
    "strip_words = lambda x: ' '.join([item for item in x.split() if item not in STOPWORDS])\n",
    "con_comments[\"body_text\"] = con_comments[\"body_text\"].apply(strip_words)\n",
    "pol_comments[\"body_text\"] = pol_comments[\"body_text\"].apply(strip_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_ZtW4U64tBj"
   },
   "source": [
    "# Data Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXJGi7oW4tBj"
   },
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq0HYDug4tBj"
   },
   "outputs": [],
   "source": [
    "def convert_to_wordlist(df):\n",
    "    # Take out the columns we don't need\n",
    "    text = df.drop([\"author\", \"score\", \"created_utc\", \"post_id\", \"body_word_count\"], axis=1)\n",
    "    words = []\n",
    "    # Turn each comment into a list of words, append to list\n",
    "    for ii in range(0,len(text)):\n",
    "        words.append(str(text.iloc[ii]['body_text']).split(\" \"))\n",
    "    # Turn nested lists into one big list\n",
    "    flat_list = [item for sublist in words for item in sublist]\n",
    "    # Final cleaning step (might not be necessary)\n",
    "    cleanedList = [x for x in flat_list if str(x) != 'nan']\n",
    "    return cleanedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11858,
     "status": "ok",
     "timestamp": 1606878450949,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "kRA0iXU94tBj",
    "outputId": "6fba5a84-1fa1-4dbd-fd40-62cd11a6f295"
   },
   "outputs": [],
   "source": [
    "con_wordlist = convert_to_wordlist(df=con_comments)\n",
    "pol_wordlist = convert_to_wordlist(df=pol_comments)\n",
    "\n",
    "con_temp_df = pd.DataFrame({'col':con_wordlist})\n",
    "pol_temp_df = pd.DataFrame({'col':pol_wordlist})\n",
    "\n",
    "print(con_wordlist[0:10])\n",
    "print(pol_wordlist[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "executionInfo": {
     "elapsed": 58354,
     "status": "ok",
     "timestamp": 1606878498544,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "4XWJVU9r4tBk",
    "outputId": "01bb6d63-bc09-4f8b-b766-fdec42628a48",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = 'white', width = 1200,  height = 1200,\n",
    "                      max_words = 150).generate(' '.join(con_wordlist))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('r/Conservative Word Cloud',fontsize = 20)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = 'white', width = 1200,  height = 1200,\n",
    "                      max_words = 150).generate(' '.join(pol_wordlist))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('r/politics Word Cloud',fontsize = 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 58531,
     "status": "ok",
     "timestamp": 1606878499708,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "zJLuAWxf4tBk",
    "outputId": "36d18014-03b9-456a-edc4-dec2bba5fb43"
   },
   "outputs": [],
   "source": [
    "n_words = 30\n",
    "rot = 45\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "color = plt.cm.copper(np.linspace(0, 1, n_words))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "con_temp_df['col'].value_counts().head(n_words).plot.bar(color = color)\n",
    "plt.title('r/Conservative Most Used Words', fontsize = 20)\n",
    "plt.xticks(rotation = rot)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "pol_temp_df['col'].value_counts().head(n_words).plot.bar(color = color)\n",
    "plt.title('r/politics Most Used Words', fontsize = 20)\n",
    "plt.xticks(rotation = rot)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duOMpO0y4tBk"
   },
   "source": [
    "# Visualizing Comment Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnIVz4zc4tBk"
   },
   "source": [
    "Set aside a copy of the comments and format them in a special way (wrap them so that a long comment doesn't show up as one super long line of text that goes off the page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4y1pQ3e54tBk"
   },
   "outputs": [],
   "source": [
    "def format_df_for_plotting(df):\n",
    "    df_formatted = df.copy(deep=True)\n",
    "    df_formatted.body_text = df_formatted.body_text.str.wrap(30)\n",
    "    df_formatted.body_text = df_formatted.body_text.apply(lambda x: x.replace('\\n', '<br>'))\n",
    "    return df_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jl2ewmgl4tBk"
   },
   "outputs": [],
   "source": [
    "con_comments_formatted = format_df_for_plotting(df=con_comments)\n",
    "pol_comments_formatted = format_df_for_plotting(df=pol_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGWdwDjd4tBk"
   },
   "source": [
    "Vectorize the comments. Here I copied the project 2 notebook and use tf-idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57174,
     "status": "ok",
     "timestamp": 1606878509118,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "H8yowV1b4tBk",
    "outputId": "623e21c8-ce8e-4b31-a244-9d841c989539"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=2**12) # Mess around with this number \n",
    "\n",
    "X_con = vectorizer.fit_transform(con_comments['body_text'].values)\n",
    "X_pol = vectorizer.fit_transform(pol_comments['body_text'].values)\n",
    "\n",
    "print(X_con.shape)\n",
    "print(X_pol.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjb--kb44tBk"
   },
   "source": [
    "## Generate the cluster labels. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZhJRx_-4tBk"
   },
   "source": [
    "#### Reduce Dimensions Using PCA\n",
    "\n",
    "\n",
    "t-SNE would take forever to compute on a N x 4096 array. So use PCA to get it down to around N x 20 (or some other number), and then use t-SNE. Might be worth experimenting with different PCA dimensions to see how it effects the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAIZ83aN4tBk"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20) \n",
    "\n",
    "con_pca_result = pca.fit_transform(X_con.toarray())\n",
    "pol_pca_result = pca.fit_transform(X_pol.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni5JNZ-WIbJW"
   },
   "source": [
    "#### Cluster Using Gaussian Mixtures\n",
    "\n",
    "Here I use Gaussian Mixtures. We can't apply it to the full N x 4096 matrix, so instead we apply it to the array we get after doing PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134459,
     "status": "ok",
     "timestamp": 1606878594316,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "WjhIpLn24tBk",
    "outputId": "00372901-a2f0-480d-fb49-f7487c28c040",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=20, n_init=1, verbose=0)\n",
    "\n",
    "y_con_pred = gm.fit_predict(con_pca_result)\n",
    "y_pol_pred = gm.fit_predict(pol_pca_result)\n",
    "print(y_pol_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJg_cmJf4tBk"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    verbose=0, \n",
    "    n_components=2,\n",
    "    perplexity=30, # good values are 10-50. 30 is default.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 670138,
     "status": "ok",
     "timestamp": 1606879133950,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "liXm-g1S4tBk",
    "outputId": "a8b1760a-323c-4e32-f687-be7609071597",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_con_embedded = tsne.fit_transform(con_pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1289950,
     "status": "ok",
     "timestamp": 1606879757159,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "n-_8qVBB4tBl",
    "outputId": "fd9e4f7f-da10-4ccf-f738-b1d81cd7e752",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_pol_embedded = tsne.fit_transform(pol_pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 817,
     "output_embedded_package_id": "1ZtSW_B36BCSqos6wkH0fu5ZBCooKdslv"
    },
    "executionInfo": {
     "elapsed": 1374882,
     "status": "ok",
     "timestamp": 1606879846724,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "_d-_OaR74tBl",
    "outputId": "f40fbb06-8a6b-45e9-95d2-ebf7a9c9d842"
   },
   "outputs": [],
   "source": [
    "# For 3D plot, change the n_components parameter of TSNE to 3. \n",
    "fig = go.Figure(\n",
    "    data=go.Scattergl( #go.Scatter3d  for 3D plot, go.Scattergl for 2D\n",
    "        name=\"\",\n",
    "        x=X_con_embedded[:,0],\n",
    "        y=X_con_embedded[:,1],\n",
    "        # z=X_con_embedded[:,2], # for 3D plot\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5, # size=2 for 3D plot, size=5 for 2D\n",
    "            opacity=0.7,\n",
    "            color = y_con_pred,\n",
    "            colorscale=\"jet\"\n",
    "        ),\n",
    "        text=con_comments_formatted['body_text'],\n",
    "        hovertemplate = \"</br> %{text}\",\n",
    "    )\n",
    ") \n",
    "\n",
    "fig.update_layout(\n",
    "    title='t-SNE r/Conservative Comments',\n",
    "    template=\"ggplot2\",\n",
    "    height=800,\n",
    "    hoverlabel=dict(\n",
    "        bgcolor=\"white\",\n",
    "        font_size=16,\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 817,
     "output_embedded_package_id": "1bz9ZKoou1EcIZXMqwepXbIS5D1XSyG31"
    },
    "executionInfo": {
     "elapsed": 1373809,
     "status": "ok",
     "timestamp": 1606879846727,
     "user": {
      "displayName": "Cameron Laedtke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdHvTpQBUhnTxCbHhpkxuEaFh6HcxY6QIb9OVX=s64",
      "userId": "09150441049660052621"
     },
     "user_tz": 360
    },
    "id": "i8WkYXIM4tBl",
    "outputId": "765c041c-6dc1-48bf-9db7-e1d2538631f9"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=go.Scattergl(\n",
    "        name=\"\",\n",
    "        x=X_pol_embedded[:,0],\n",
    "        y=X_pol_embedded[:,1],\n",
    "        # z=X_pol_embedded[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            opacity=0.7,\n",
    "            color = y_pol_pred,\n",
    "            colorscale=\"jet\"\n",
    "        ),\n",
    "        text=pol_comments_formatted['body_text'],\n",
    "        hovertemplate = \"</br> %{text}\",\n",
    "    )\n",
    ") \n",
    "\n",
    "fig.update_layout(\n",
    "    title='t-SNE r/politics Comments',\n",
    "    template=\"ggplot2\",\n",
    "    height=800,\n",
    "    hoverlabel=dict(\n",
    "        bgcolor=\"white\",\n",
    "        font_size=16,\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxMSmPhj4tBl"
   },
   "source": [
    "Here I made labels 1 for conservative and 0 for liberal. Use r/democrats or r/politics for the \"liberal\" label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mzmyQddi4tBl"
   },
   "outputs": [],
   "source": [
    "# Labels for conservative or democrat\n",
    "con_comments[\"label\"] = 1\n",
    "pol_comments[\"label\"] = 0\n",
    "\n",
    "df = pd.concat([con_comments, pol_comments])\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNH0EzfTMESL"
   },
   "source": [
    "Sample 15% of the data for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4JDEi8mwMQUz"
   },
   "outputs": [],
   "source": [
    "test_df = df.sample(frac=0.15, replace=False)\n",
    "\n",
    "test_df.to_pickle(\"reddit_data/TEST_DATASET.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNq3Hu9ZNdOp"
   },
   "source": [
    "Save the remaining data as the training set. The training set can then be split into training and validation sets when experimenting with models. Only use the test set for final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1NfXvaq34tBm"
   },
   "outputs": [],
   "source": [
    "df.to_pickle(\"reddit_data/DATASET.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPgSWskZN7k5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "PREPROCESSING.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
